# Evaluation

{/* introduce log10's approach to evaluation */}
{/* llmeval was declarative, making small report generation easy */}
{/* testing multi step, agentic systems with tool calling and context retrieval is better done in code than a declarative configuration language */}
{/* dataset needs processing, parameters needs sweeping */}
{/* Get in touch for other lasnguage support, such as typescript */}

{/* TODO: Architecture diagram */}

## Installation

Follow [this guide](/observability/advanced/logging) to get set up with log10 in your local environment.
After you have verified that log10 can capture logs from your application, you can install the evaluation package.

```bash
$ mkdir eval_test && cd eval_test
$ virtualenv venv
$ source venv/bin/activate
$ pip install log10[pytest]
```

**NOTE** We recommend using a virtual environment to install the package, as other pytest runs may be captured and sent to log10.

**NOTE** If you are not seeing runs or logs, it may be because the pytest is the system wide one. You can check which one is being used with `which pytest`.

## Getting started

Start by creating your first test file, `test_example.py`.

```python
import pytest

def test_example():
    assert 0 == 1
```

Run the test with the following command:

```bash
$ pytest test_example.py
```

You should see something like this:

```bash
============================================================================== test session starts ===============================================================================
platform darwin -- Python 3.11.10, pytest-8.3.3, pluggy-1.5.0
rootdir: /Users/niklasqnielsen/workspace/log10/eval-v2
plugins: json-report-1.5.0, metadata-3.1.1, anyio-4.6.0
collected 1 item                                                                                                                                                                 

tests/simple/test_getting_started.py F                                                                                                                                     [100%]

==================================================================================== FAILURES ====================================================================================
__________________________________________________________________________________ test_example __________________________________________________________________________________

    def test_example():
>       assert 0 == 1
E       assert 0 == 1

tests/simple/test_getting_started.py:4: AssertionError
---------------------------------------------------------------------------------- JSON report -----------------------------------------------------------------------------------
report saved to: eval-v2-86f68600-af07-4199-ab76-ceb81685f053.report.json
Report successfully uploaded to Log10
--------------------------------------------------------------------------------- Log10 test run ---------------------------------------------------------------------------------
Log10 test run is enabled. Test run: eval-v2-86f68600-af07-4199-ab76-ceb81685f053
============================================================================ short test summary info =============================================================================
FAILED tests/simple/test_getting_started.py::test_example - assert 0 == 1
=============================================================================== 1 failed in 0.23s ================================================================================
```

And if you follow the link above, you should see the test and the call in the log10 dashboard.

{/* Insert screenshot */}

Now, let's make an LLM call and see the test and the call in the log10 dashboard.

```python
import pytest
from log10.load import OpenAI


def test_example():
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": "What is the capital of France?"}],
    )

    # Paris should be in the response, but may have surrounding text, end with a period etc.
    assert "paris" in response.choices[0].message.content.lower()
```

Run the test again:

```bash
$ pytest test_example.py
================================================================== test session starts ==================================================================
platform darwin -- Python 3.11.10, pytest-8.3.3, pluggy-1.5.0
rootdir: /Users/niklasqnielsen/workspace/log10/eval-v2
plugins: json-report-1.5.0, metadata-3.1.1, anyio-4.6.0
collected 1 item                                                                                                                                        

tests/simple/test_getting_started.py .                                                                                                            [100%]

---------------------------------------------------------------------- JSON report ----------------------------------------------------------------------
report saved to: eval-v2-b0979047-9acd-4582-bcde-868c1d0b9820.report.json
Report successfully uploaded to Log10
-------------------------------------------------------------------- Log10 test run ---------------------------------------------------------------------
Log10 test run is enabled. Test run: eval-v2-b0979047-9acd-4582-bcde-868c1d0b9820
============================================================= 1 passed, 1 warning in 1.50s ==============================================================
```

You should see the test and the call in the log10 dashboard.

{/* Insert screenshot */}

## Next steps

Now that you have a basic test running, you can start to explore different ways to test your application.

- [Loading data](/evaluation/loading)
- [Sweeping hyperparameters](/evaluation/sweep)
- [Exploring metrics](/evaluation/metrics)
- [Testing tool and multi-step calls](/evaluation/tools)
- [Testing agentic applications](/evaluation/agents)
- [Using AutoFeedback for human in the loop evaluations](/evaluation/human)
- [Best practices](/evaluation/best-practices)