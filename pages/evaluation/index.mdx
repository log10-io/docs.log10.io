# Evaluation

Evaluation is easily one of the most important practices when building production grade AI systems.
Without it, you can't be sure that your system is working as intended as you grow from a prototype to a production system.
When new models are available, implementing new features and accomodating new customers, you need to be able to quickly and confidently evaluate the impact of these changes.

At log10, we built one of the first declarative evaluation systems, [LLM Eval](/evaluation/llmeval_v1/installation), which let users define their evaluation in configuration files, run prompts across models and datasets, and generate reports with a single command.

While this approach was powerful, it was also limited in its flexibility when it comes to testing advanced use-cases with multi-step, agentic systems, tool calling, and context retrieval.
Further more, our customers requested ways to bootstrap their evaluations with captured logs and feedback from human evaluators. Requiring a more flexible approach to evaluation.

In this new version, we are introducing a more flexible, code-based approach to evaluation which integrates seamlessly with log10's logging capabilities.

{/* Get in touch for other lasnguage support, such as typescript */}
{/* TODO: Architecture diagram */}

## Installation

Follow [this guide](/observability/advanced/logging) to get set up with log10 in your local environment.
After you have verified that log10 can capture logs from your application, you can install the evaluation package.

```bash
$ mkdir eval_test && cd eval_test
$ virtualenv venv
$ source venv/bin/activate
$ pip install log10[pytest]
```

**NOTE** We recommend using a virtual environment to install the package, as other pytest runs may be captured and sent to log10.

**NOTE** If you are not seeing runs or logs, it may be because the pytest is the system wide one. You can check which one is being used with `which pytest`.

## Getting started

Start by creating your first test file, `test_example.py`.

```python
import pytest

def test_example():
    assert 0 == 1
```

Run the test with the following command:

```bash
$ pytest test_example.py
```

You should see something like this:

```bash
============================================================================== test session starts ===============================================================================
platform darwin -- Python 3.11.10, pytest-8.3.3, pluggy-1.5.0
rootdir: /Users/niklasqnielsen/workspace/log10/eval-v2
plugins: json-report-1.5.0, metadata-3.1.1, anyio-4.6.0
collected 1 item                                                                                                                                                                 

tests/simple/test_getting_started.py F                                                                                                                                     [100%]

==================================================================================== FAILURES ====================================================================================
__________________________________________________________________________________ test_example __________________________________________________________________________________

    def test_example():
>       assert 0 == 1
E       assert 0 == 1

tests/simple/test_getting_started.py:4: AssertionError
---------------------------------------------------------------------------------- JSON report -----------------------------------------------------------------------------------
report saved to: eval-v2-86f68600-af07-4199-ab76-ceb81685f053.report.json
Report successfully uploaded to Log10
--------------------------------------------------------------------------------- Log10 test run ---------------------------------------------------------------------------------
Log10 test run is enabled. Test run: eval-v2-86f68600-af07-4199-ab76-ceb81685f053
============================================================================ short test summary info =============================================================================
FAILED tests/simple/test_getting_started.py::test_example - assert 0 == 1
=============================================================================== 1 failed in 0.23s ================================================================================
```

And if you follow the link above, you should see the test and the call in the log10 dashboard.

{/* Insert screenshot */}

Now, let's make an LLM call and see the test and the call in the log10 dashboard.

```python
import pytest
from log10.load import OpenAI


def test_example():
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": "What is the capital of France?"}],
    )

    # Paris should be in the response, but may have surrounding text, end with a period etc.
    assert "paris" in response.choices[0].message.content.lower()
```

Run the test again:

```bash
$ pytest test_example.py
================================================================== test session starts ==================================================================
platform darwin -- Python 3.11.10, pytest-8.3.3, pluggy-1.5.0
rootdir: /Users/niklasqnielsen/workspace/log10/eval-v2
plugins: json-report-1.5.0, metadata-3.1.1, anyio-4.6.0
collected 1 item                                                                                                                                        

tests/simple/test_getting_started.py .                                                                                                            [100%]

---------------------------------------------------------------------- JSON report ----------------------------------------------------------------------
report saved to: eval-v2-b0979047-9acd-4582-bcde-868c1d0b9820.report.json
Report successfully uploaded to Log10
-------------------------------------------------------------------- Log10 test run ---------------------------------------------------------------------
Log10 test run is enabled. Test run: eval-v2-b0979047-9acd-4582-bcde-868c1d0b9820
============================================================= 1 passed, 1 warning in 1.50s ==============================================================
```

You should see the test and the call in the log10 dashboard.

{/* Insert screenshot */}

## Next steps

Now that you have a basic test running, you can start to explore different ways to test your application.

- [Loading data](/evaluation/loading)
- [Sweeping hyperparameters](/evaluation/sweep)
- [Exploring metrics](/evaluation/metrics)
- [Testing tool and multi-step calls](/evaluation/tools)
- [Testing agentic applications](/evaluation/agents)
- [Using AutoFeedback for human in the loop evaluations](/evaluation/human)
- [Best practices](/evaluation/best-practices)