# Tutorial: How to get user feedback in a RAG system

A common use case of AI agents is to provide answers to user questions. Once the system has provided an answer, you might want to collect feedback from the user about the quality of the response or have your support staff rate AI-generated answers.

To demonstrate how Log10 integrates with an OpenAI chatbot to log completions and feedback, this tutorial will show you how a user of a customer support system can ask a question and then rate the generated answer.

We'll mock out parts of a retrieval-augmented generation (RAG) system, focusing on how to define a feedback task and collect user feedback as part of an end-to-end system in Log10.

If you'd like to follow along, ensure Log10 is set up correctly. Follow the [getting started guide](./getting_started) to complete the initial setup.

At the end of this tutorial, you'll have a command-line app that looks like this.

![Log10 feedback app](/images/docs/feedback/tutorial/app.png)

## Importing libraries

Create a file called `rag.py` in your local directory and add the following code to it.

```python
import uuid
from dotenv import load_dotenv

from log10.load import OpenAI

from log10.feedback.feedback import Feedback
from log10.feedback.feedback_task import FeedbackTask

load_dotenv()
```

Here we load in all the environment variables to make sure you can connect to Log10 and your LLM provider.

## Getting chat completions from OpenAI

To test that OpenAI and Log10 are configured correctly, let's create a test completion.

First, initialize a default OpenAI client and specify the default model to use. This step is optional, but doing it will save you from typing or pasting the model name with each completion call.

Add the following code to your file. 

```python
feedback_id=str(uuid.uuid4())
client = OpenAI(tags=[feedback_id])
DEFAULT_MODEL = "gpt-3.5-turbo-0125"

response = client.chat.completions.create(
    model = DEFAULT_MODEL,
    messages = [{"role":"user", "content":"What are some of the most noteworthy archaeological dig sites around the world?"}]
)

print(response.choices[0].message.content)
```

Note that we use the OpenAI client from the Log10 library, so all OpenAI interactions will automatically log to your Log10 account.

We add a unique `feedback_id` tag to the client to use later when we log feedback from users.

Now run the script. 

In the Log10 dashboard, navigate to the Logs page. You should see your test completion has just been logged.

![Log10 dashboard displaying a successful completion](/images/docs/feedback/tutorial/log.png)

## Reading a PDF

Now that we have tested the OpenAI and Log10 setups, we can get started on creating a system that can parse information from a PDF document.

Typically, this process involves many steps. First, we would load in the PDF file. Then the contents of the file would be split into small chunks that are vectorized and stored in a vector database.

When a user makes a query, it is vectorized and the database is searched for similar vectors. Any chunks of text that are relevant to the query are returned.

The results of the vector lookup are then turned back into plain text and passed to the LLM along with the query as supplemental context. The LLM uses all the context it has been provided to generate a response accordingly.

To keep the focus of this tutorial on Log10, we'll mock out some parts of this process, but you should be able to adapt this to integrate with your real-world RAG system.

To start, we'll define a function to receive the user inquiry and output relevant context. 

Suppose a user wants to know more about the disability benefits their health insurance provides. The "parser" has searched the provided document and returns all relevant information.

Remove the following lines from your `rag.py` file:

```
response = client.chat.completions.create(
    model = DEFAULT_MODEL,
    messages = [{"role":"user", "content":"What are some of the most noteworthy archaeological dig sites around the world?"}]
)

print(response.choices[0].message.content)
```

Replace the deleted lines with this code:

```python
def get_context(msg):
    # mocking a context retriever

    context = [
        "DISABILITY BENEFIT. A lump-sum payout that depends on the category of the disability and on your selected benefit option.",
        "The Disability Benefit offers cover that ensures the financial security of your family with a lump-sum payout if you become disabled.",
        "You can select the A or B Disability Benefits, which provide an additional payout of up to 200 percent and 100 percent respectively.",
        "Permanent disability: Pays out 100 percent if the disability meets the objective medical or Activities of Daily Living criteria.",
        "Partial permanent disability: Pays out 50 percent if the disability meets the objective medical or Activities of Daily Living criteria."
    ]

    return context
```

## Supplying conversation context

Now we have all the relevant context from the PDF document and we can pass it to the API. 

First, we tell GPT to reply based on the provided context. Add the following function to your `rag.py` file.

```python
def pass_context(context):
    msg = f"Use only this context to answer any following questions: \n {context}"

    return msg
```

This function accepts the context as input and formats it into a string that we can pass to the API.

The API does not keep track of conversation history - each inquiry is treated independently. However, we want the user to hold a coherent conversation with the chatbot, so we need to keep track of the conversation history and pass it to the API with each query. 

Add the following function to your `rag.py` file to keep track of the conversation history. 

```python
def query(context, chat_messages):
    context_msg = [{"role": "user", "content": pass_context(context)}]
    chat_msgs = [
        {"role": "assistant", "content": chat_messages[i]}
        if i % 2
        else {"role": "user", "content": chat_messages[i]}
        for i in range(len(chat_messages))
    ]
    msgs = context_msg + chat_msgs
    response = client.chat.completions.create(model=DEFAULT_MODEL, messages=msgs)

    return response.choices[0].message.content
```

Here's what this function does:

- Receives the document context and a list of chat messages as input.
- Formats all messages with the `role` and `content` keys in a dictionary. All messages passed to the API must be formatted like this.
- Calls the `pass_context` function we defined earlier and formats it as a user message.
- Loops through the provided chat history and formats the chat messages. Responses from the chatbot are classified as `assistant` messages, while the user queries are classified as `user`.
- Merges the context message with the list of user messages.
- Makes the API call and returns the response.

Note that this function assumes that each conversation will consist of alternating messages between the user and the assistant.

## Testing our RAG system

Let's question our mock PDF document.

We define a prompt and get relevant context for it. Add the following code to your `rag.py` file.

```python
def main():
    prompt = "What disability benefits are included in my plan?"
    context = get_context(prompt)
    chat_messages = [prompt]
    response = query(context, chat_messages)
    print(response)

if __name__ == '__main__':
    main()
```

For our purposes, the same context is returned no matter what the prompt is. In a real-world setting, the information would be found in a vector database and the relevant context returned.

After running the script, you can view the logs in your Log10 dashboard.

![Message history](/images/docs/feedback/tutorial/message_history.png)

## Defining a feedback task

Now we can incorporate feedback logging. 

In a real-time, interactive chatbot setting, feedback could take the form of a customer satisfaction prompt at the end of a conversation or an internal agent rating the quality of responses provided by the LLM.

This tutorial won't cover how to collect feedback from the user. Instead, we'll focus on how to log feedback to Log10 and link it to the correct completions.

First we'll define a feedback task to capture information like the expected format and default values. In this example, we create a user satisfaction rating.

Add the following feedback options near the top of your RAG file, above the first function definition.

```python
FEEDBACK_OPTIONS = {
    "1": "Very Satisfied",
    "2": "Satisfied",
    "3": "Neutral",
    "4": "Dissatisfied",
    "5": "Very Dissatisfied",
}
```

Then add the following `create_feedback` function.

```python
def create_feedback():
    task = FeedbackTask().create(
        name="Satisfaction Rating",
        task_schema={
            "$schema": "http://json-schema.org/draft-07/schema#",
            "title": "Satisfaction Rating",
            "description": "Choose out of five satisfaction levels",
            "type": "string",
            "enum": list(FEEDBACK_OPTIONS.values()),
        },
    )
    return task.json()["id"]
```

## Logging feedback

Now that you've defined the feedback task, you can collect feedback. 

Update your `main` function to look like the code below.

```python
def main():
    prompt = input("What's your question? ")
    context = get_context(prompt)
    chat_messages = [prompt]
    response = query(context, chat_messages)
    print(response)

    task_id = create_feedback()
    feedback_display = "\n".join(
        f"{key}: {value}" for key, value in FEEDBACK_OPTIONS.items()
    )

    user_input = None
    while not user_input in FEEDBACK_OPTIONS.keys():
        user_input = input(
            f"Please select a number indicating how satisfied you were with this response.\n"
            + feedback_display
            + "\n\n>"
        )

    Feedback().create(
        task_id=task_id,
        values=FEEDBACK_OPTIONS[user_input],
        completion_tags_selector=[feedback_id],
    )

    print("Thank you, your feedback has been recorded")
```

This function accepts a rating between 1 and 5 from the user and passes the associated description to Log10.

Run the script, enter the prompt, and give feedback. 

From the Log10 dashboard, navigate to the Feedback page and you will see your newly logged feedback.

![Logged feedback](/images/docs/feedback/tutorial/feedback.png)



